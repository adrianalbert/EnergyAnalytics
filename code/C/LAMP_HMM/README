 =============================================================================
							README file for LAMP_HMM Code
 =============================================================================

 PACKAGE: LAMP_HMM version 0.9
      
 Authors: Daniel DeMenthon & Marc Vuilleumier
 
 email: daniel@cfar.umd.edu
 http://cfar.umd.edu/~daniel

 Author for starting point C code: Tapas Kanungo (see below).
   
 Date:   February, 1999. Revision January 2003

 Modification Log:
 January 5, 2003: Process observation vectors with discrete pdfs 
                    and different nbSymbols for each components
 January 9, 2003: Read input data from a config file
 
 Organization: 	Language and Media Processing (LAMP)
               		University of Maryland
               		College Park, MD  20742

=============================================================================

Content:

This package contains C++ code for applying  hidden Markov models (HMM) to files of observation data. 

This code used Tapas Kanungo's C code as a starting point.
(http://www.cfar.umd.edu/~kanungo/software/software.html)
You will recognize some of the original code, mainly in hmm.C
You will also recognize the file format for the output HMM data.

As for Tapas's code, the notation used is very similar to that used by Rabiner and Juang in:

- Rabiner, L. R. and B. H. Juang, "Fundamentals of Speech Recognition,"
  Prentice Hall, 1993.
- Rabiner, L. R., "A Tutorial on Hidden Markov Models and Selected 
  Applications in Speech Recognition, Prov. of IEEE, vol. 77, no. 2, 
  pp. 257-286, 1989.
- Rabiner, L. R., and B. H. Juang, "An Introduction to Hidden Markov Models,"
  IEEE ASSP Magazine, vol. 3, no. 1, pp. 4-16, Jan. 1986. 
  

Main upgrades from Tapas Kanungo's code:

1. Observations can be vectors. See obs.h for types and methods.
Thanks to polymorphism in C++, the code in hmm.C can deal with various types
of observations, observation models and transition models  without modifications.
The drawback is that there is quite a bit of type casting; also, debuggers can't trace the
code's progress when dynamic linking takes place. But breaks still work: to debug, 
place breaks in the class where you think the code is going to go, and execution will stop there.

2. Training can be accomplished by multiple observation sequences, defined in a
*.seq file.  Look at lenna64.seq for an example. There, observations are RGB
triples, so in the observation sequence the 3 values are listed one after the
other. For 100 observations there are 300 numbers. The format for a file of observation
sequences is
P6 [a file ID]
nbSequences= 200 [number of observation sequences coming next]
T= 25 [number of feature vectors in first sequence; each vector has 3 components here, 
so there would be 75 numbers next]
242 192 137 197 93 75 242 192 137 245 190 130 242 192 137 151 63 80 ...
T= 675 [number of feature vectors for second sequence]
etc...
Note that the number of components for each feature vector is not specified in the *seq file,
and is only specified in the config file under the label nbDimensions=
If you have nbDimensions= 4 and the *.seq file has a list of feature vectors of dimension 3, the 
program will abort when reading the file.

3. Observation probabilities can be modeled by histograms or gaussians.  

4. State durations can be modeled explicitly or implicitly (see Rabiner-Juang book for
discussion about distinction).

5. In addition to Baum-Welch, the much faster segmental k-means method and a hybrid method are
implemented for training. Results are similar.


=============================================================================

Installation:

  UNIX: Mac OS X, Sun Solaris:

  Type "make hmmFind" at the unix prompt. This should
  compile the package.
  
  I also built an application using Mac OS X Project Builder. 
  I did not check with Linux or Visual C++. Let me know.
  
=============================================================================

Usage: ./hmmFind <configFileName>
Example: ./hmmFind ./configFile.txt

There is a single executable. You switch from training to processing new observation data
by editing the configFile:

=============================================================================

configFile contains comment lines and required input data, which are:

- Name for sequence file *.seq containing number of sequences and observation sequences,
- The switch skipLearning is 0 if we are learning a new HMM
and 1 for finding -log Prob(observation|model) (for classification tasks).
- Name for file containing input HMM. 
  This HMM is used as an initialization for finding HMM of data if skipLearning is 0
  and used for finding -log Prob(observation|model) if skipLearning is 1.
- Name for prefix used to produce four output files,
1. prefix.hmm: computed HMM model
2. prefix.sta: sequences of most probable states using Viterbi
3. prefix.obs: sequences of expected observations for each probable states
4. prefix.dis: distance of result HMM or input HMM to each of the sequences
- Input data in config file also include feature vector dimension, nbs of symbols and nb of states,
and switches that specify the type of modeling to be used.

=============================================================================

main() is in hmmFind.C

=============================================================================

IMPORTANT NOTE:

The arrays used for computing histograms are one-based. Therefore, symbols used in
observation vector components should be chosen in the range 1 to M, and cannot be zero.
For example, if you observe grey levels in a random walk on an image, you will encounter
regions where the pixels are black and the grey level is 0. Add 1 to the observed
grey levels to obtain a symbol range from 1 to 256. Remember to subtract 1 if for example
you want to generate an image of expected observations.


=============================================================================
=============================================================================

Notes about the combination of multiple observation sequences:

In page 370 of Rabiner-Juang book, this issue is discussed; but the 1/Pk terms should have disappeared,
cancelled out by the scaling factors in alpha and beta. So if we don't
rescale the numerators and denominators of A and B when we cumulate them, we
will automatically find the results for aij and bij corresponding to the
expressions (6.110) and (6.111) of the book.

=============================================================================

- Daniel DeMenthon, January 14, 2003